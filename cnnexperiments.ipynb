{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import lightning as L\n",
    "import torchmetrics\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import Imagenette\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Prepare the dataset\n",
    "train_transforms = transforms.Compose([\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    # transforms.RandomCrop(160, padding=8, padding_mode='reflect'),  # Data Augmentation\n",
    "    transforms.CenterCrop(160),\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "    transforms.Grayscale()\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.CenterCrop(160),\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "    transforms.Grayscale()\n",
    "])\n",
    "\n",
    "train_dataset = Imagenette(\"data/imagenette/train/\", split=\"train\", size=\"160px\",  transform=train_transforms)\n",
    "\n",
    "# Use 10% of the training set for validation\n",
    "train_set_size = int(len(train_dataset) * 0.9)\n",
    "val_set_size = len(train_dataset) - train_set_size\n",
    "\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_set_size, val_set_size], generator=seed)\n",
    "val_dataset.dataset.transform = test_transforms\n",
    "\n",
    "# Use DataLoader to load the dataset\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, num_workers=8, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128, num_workers=8, shuffle=False)\n",
    "\n",
    "# Configure the test dataset\n",
    "test_dataset = Imagenette(\"data/imagenette/test/\", split=\"val\", size=\"160px\",  transform=test_transforms)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, num_workers=8, shuffle=False)\n",
    "\n",
    "# Add EarlyStopping\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\",\n",
    "                                    mode=\"min\",\n",
    "                                    patience=5)\n",
    "\n",
    "# Configure Checkpoints\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for images, labels in train_loader:\n",
    "#   # Get the shape of the first image in the batch\n",
    "#   image_shape = images[0].shape\n",
    "#   print(f\"Image shape: {image_shape}\")\n",
    "#   break  # You only need to check the shape once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment -0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(L.LightningModule):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.estimator = nn.Sequential(\n",
    "            nn.Linear(64 * 64, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "        self.train_loss=[]\n",
    "        self.val_loss=[]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        return self.estimator(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "        self.log(\"train_loss\", loss,prog_bar=True)\n",
    "        self.train_loss.append(loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "        self.accuracy(y_hat, y)\n",
    "\n",
    "        self.val_loss.append(loss)\n",
    "\n",
    "\n",
    "        self.log(\"val_accuracy\", self.accuracy,prog_bar=True)\n",
    "        self.log(\"val_loss\", loss,prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "        self.accuracy(y_hat, y)\n",
    "\n",
    "        self.log(\"test_accuracy\", self.accuracy)\n",
    "        self.log(\"test_loss\", loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\anaconda3\\envs\\cse6363\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "d:\\anaconda3\\envs\\cse6363\\Lib\\site-packages\\lightning\\pytorch\\loops\\utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | estimator | Sequential         | 4.8 M \n",
      "1 | accuracy  | MulticlassAccuracy | 0     \n",
      "-------------------------------------------------\n",
      "4.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.8 M     Total params\n",
      "19.148    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 67/67 [01:09<00:00,  1.04s/it, v_num=20, train_loss=0.813, val_accuracy=0.348, val_loss=2.370]\n"
     ]
    }
   ],
   "source": [
    "model = BaselineModel()\n",
    "\n",
    "# Add EarlyStopping\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\",\n",
    "                                    mode=\"min\",\n",
    "                                    patience=5)\n",
    "\n",
    "# Configure Checkpoints\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "trainer = L.Trainer(callbacks=[early_stop_callback, checkpoint_callback])\n",
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model stored at d:\\UTA\\Sem-2\\ML\\CSE6363\\assignments\\assignment4\\lightning_logs\\version_20\\checkpoints\\epoch=3-step=268.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 31/31 [00:03<00:00,  8.39it/s]\n",
      "Train Test result: [{'test_accuracy': 0.3314649760723114, 'test_loss': 1.9839969873428345}]\n",
      "Best validation loss 1.9318835735321045\n",
      "Test loss: 1.9839969873428345\n",
      "Test Accuracy: 0.3314649760723114\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best model stored at {trainer.checkpoint_callback.best_model_path}\")\n",
    "\n",
    "## Restoring the best checkpoint\n",
    "model = BaselineModel.load_from_checkpoint(\n",
    "            trainer.checkpoint_callback.best_model_path\n",
    "        )\n",
    "##Obtaining the test metrics\n",
    "test_result = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
    "print(f\"Train Test result: {test_result}\")\n",
    "\n",
    "print(f\"Best validation loss {trainer.checkpoint_callback.best_model_score.item()}\\nTest loss: {test_result[0]['test_loss']}\\nTest Accuracy: {test_result[0]['test_accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(BaselineModel):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.estimator = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # Input channel = 1 for grayscale\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),  # Flatten output for fully connected layers\n",
    "            nn.Linear(64 * 16 * 16, 128),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.view(x.shape[0], 1, 64, 64)  # Reshape for CNN (1 channel for grayscale)\n",
    "        \n",
    "        return self.estimator(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\anaconda3\\envs\\cse6363\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "d:\\anaconda3\\envs\\cse6363\\Lib\\site-packages\\lightning\\pytorch\\loops\\utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | estimator | Sequential         | 2.1 M \n",
      "1 | accuracy  | MulticlassAccuracy | 0     \n",
      "-------------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 M     Total params\n",
      "8.470     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 67/67 [01:07<00:00,  1.00s/it, v_num=11]         \n"
     ]
    }
   ],
   "source": [
    "model = CNNModel()\n",
    "\n",
    "# Add EarlyStopping\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\",\n",
    "                                    mode=\"min\",\n",
    "                                    patience=5)\n",
    "\n",
    "# Configure Checkpoints\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "trainer = L.Trainer(callbacks=[early_stop_callback, checkpoint_callback])\n",
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 31/31 [00:01<00:00, 26.95it/s]\n",
      "Train Test result: [{'test_accuracy': 0.571974515914917, 'test_loss': 1.3563098907470703}]\n",
      "Best validation loss 1.3125014305114746\n",
      "Test loss: 1.3563098907470703\n",
      "Test Accuracy: 0.571974515914917\n"
     ]
    }
   ],
   "source": [
    "## Restoring the best checkpoint\n",
    "model = CNNModel.load_from_checkpoint(\n",
    "            trainer.checkpoint_callback.best_model_path\n",
    "        )\n",
    "##Obtaining the test metrics\n",
    "test_result = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
    "print(f\"Train Test result: {test_result}\")\n",
    "\n",
    "print(f\"Best validation loss {trainer.checkpoint_callback.best_model_score.item()}\\nTest loss: {test_result[0]['test_loss']}\\nTest Accuracy: {test_result[0]['test_accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllConvNet(BaselineModel):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.estimator = nn.Sequential(\n",
    "            nn.Conv2d(1, 96, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(96, 96, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(96, 96, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(96, 192, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(192, 192, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(192, 192, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(192, 192, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(192, 192, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(192, 10, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.estimator(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | estimator | Sequential         | 1.4 M \n",
      "1 | accuracy  | MulticlassAccuracy | 0     \n",
      "2 | avg_pool  | AdaptiveAvgPool2d  | 0     \n",
      "3 | softmax   | Softmax            | 0     \n",
      "-------------------------------------------------\n",
      "1.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 M     Total params\n",
      "5.472     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 67/67 [01:19<00:00,  1.19s/it, v_num=12]        \n"
     ]
    }
   ],
   "source": [
    "model = AllConvNet()\n",
    "\n",
    "# Add EarlyStopping\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\",\n",
    "                                    mode=\"min\",\n",
    "                                    patience=5)\n",
    "\n",
    "# Configure Checkpoints\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "trainer = L.Trainer(callbacks=[early_stop_callback, checkpoint_callback])\n",
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 31/31 [00:02<00:00, 13.66it/s]\n",
      "Best validation loss 1.9683494567871094\n",
      "Test loss: 1.9843178987503052\n",
      "Test Accuracy: 0.4736305773258209\n"
     ]
    }
   ],
   "source": [
    "## Restoring the best checkpoint\n",
    "model = AllConvNet.load_from_checkpoint(\n",
    "            trainer.checkpoint_callback.best_model_path\n",
    "        )\n",
    "##Obtaining the test metrics\n",
    "test_result = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
    "\n",
    "print(f\"Best validation loss {trainer.checkpoint_callback.best_model_score.item()}\\nTest loss: {test_result[0]['test_loss']}\\nTest Accuracy: {test_result[0]['test_accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModelwithDropOut(BaselineModel):\n",
    "    def __init__(self, num_classes=10,img_shape=(1,64,64)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.estimator = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # Input channel = 1 for grayscale\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(p=0.3),  # Add dropout after max pooling\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(p=0.3),  # Add dropout after max pooling\n",
    "            nn.Flatten(),  # Flatten output for fully connected layers\n",
    "            nn.Linear(64 * 16 * 16, 128),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.img_shape = img_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.view(x.shape[0], self.img_shape[0], self.img_shape[1], self.img_shape[2])  # Reshape for CNN (1 channel for grayscale)\n",
    "        \n",
    "        return self.estimator(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\anaconda3\\envs\\cse6363\\Lib\\site-packages\\lightning\\pytorch\\loops\\utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | estimator | Sequential         | 2.1 M \n",
      "1 | accuracy  | MulticlassAccuracy | 0     \n",
      "-------------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 M     Total params\n",
      "8.470     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 67/67 [01:05<00:00,  1.02it/s, v_num=15]        \n"
     ]
    }
   ],
   "source": [
    "model = CNNModelwithDropOut()\n",
    "\n",
    "# Add EarlyStopping\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\",\n",
    "                                    mode=\"min\",\n",
    "                                    patience=5)\n",
    "\n",
    "# Configure Checkpoints\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "trainer = L.Trainer(callbacks=[early_stop_callback, checkpoint_callback])\n",
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 31/31 [00:01<00:00, 26.09it/s]\n",
      "Train Test result: [{'test_accuracy': 0.5467516183853149, 'test_loss': 1.3912665843963623}]\n",
      "Best validation loss 1.3289514780044556\n",
      "Test loss: 1.3912665843963623\n",
      "Test Accuracy: 0.5467516183853149\n"
     ]
    }
   ],
   "source": [
    "## Restoring the best checkpoint\n",
    "print(f\"Restoring from {trainer.checkpoint_callback.best_model_path}\")\n",
    "\n",
    "model = CNNModelwithDropOut.load_from_checkpoint(\n",
    "            trainer.checkpoint_callback.best_model_path\n",
    "        )\n",
    "##Obtaining the test metrics\n",
    "test_result = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
    "\n",
    "print(f\"Best validation loss {trainer.checkpoint_callback.best_model_score.item()}\\nTest loss: {test_result[0]['test_loss']}\\nTest Accuracy: {test_result[0]['test_accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Downloading CIFAR-10 dataset\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "# Prepare the dataset\n",
    "train_transforms = transforms.Compose([\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    # transforms.RandomCrop(160, padding=8, padding_mode='reflect'),  # Data Augmentation\n",
    "    transforms.CenterCrop(160),\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "    transforms.Grayscale()\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.CenterCrop(160),\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "    transforms.Grayscale()\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = CIFAR10(\"data/cifar/train/\", train =True,  transform=train_transforms,download=True)\n",
    "\n",
    "# Use 10% of the training set for validation\n",
    "train_set_size = int(len(train_dataset) * 0.9)\n",
    "val_set_size = len(train_dataset) - train_set_size\n",
    "\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_set_size, val_set_size], generator=seed)\n",
    "val_dataset.dataset.transform = test_transforms\n",
    "\n",
    "# Use DataLoader to load the dataset\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, num_workers=8, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128, num_workers=8, shuffle=False)\n",
    "\n",
    "# Configure the test dataset\n",
    "test_dataset = CIFAR10(\"data/cifar/test/\", train =False,  transform=test_transforms,download=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, num_workers=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\anaconda3\\envs\\cse6363\\Lib\\site-packages\\lightning\\pytorch\\loops\\utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | estimator | Sequential         | 2.1 M \n",
      "1 | accuracy  | MulticlassAccuracy | 0     \n",
      "-------------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 M     Total params\n",
      "8.470     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|██████████| 352/352 [01:29<00:00,  3.95it/s, v_num=16]      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 79/79 [00:01<00:00, 43.88it/s]\n",
      "Best validation loss 1.173889398574829\n",
      "Test loss: 1.178741693496704\n",
      "Test Accuracy: 0.5845999717712402\n"
     ]
    }
   ],
   "source": [
    "# Model from scratch without fine tuning\n",
    "\n",
    "model = CNNModelwithDropOut()\n",
    "\n",
    "# Add EarlyStopping\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\",\n",
    "                                    mode=\"min\",\n",
    "                                    patience=5)\n",
    "\n",
    "# Configure Checkpoints\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "trainer = L.Trainer(callbacks=[early_stop_callback, checkpoint_callback])\n",
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "\n",
    "## Restoring the best checkpoint\n",
    "model = CNNModelwithDropOut.load_from_checkpoint(\n",
    "            trainer.checkpoint_callback.best_model_path\n",
    "        )\n",
    "##Obtaining the test metrics\n",
    "test_result = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
    "\n",
    "print(f\"Best validation loss {trainer.checkpoint_callback.best_model_score.item()}\\nTest loss: {test_result[0]['test_loss']}\\nTest Accuracy: {test_result[0]['test_accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\anaconda3\\envs\\cse6363\\Lib\\site-packages\\lightning\\pytorch\\loops\\utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | estimator | Sequential         | 2.1 M \n",
      "1 | accuracy  | MulticlassAccuracy | 0     \n",
      "-------------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 M     Total params\n",
      "8.470     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 352/352 [01:24<00:00,  4.16it/s, v_num=19]      \n",
      "Restoring from d:\\UTA\\Sem-2\\ML\\CSE6363\\assignments\\assignment4\\lightning_logs\\version_19\\checkpoints\\epoch=19-step=7040.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 79/79 [00:01<00:00, 51.91it/s]\n",
      "Best validation loss 1.319576621055603\n",
      "Test loss: 1.3074554204940796\n",
      "Test Accuracy: 0.5364000201225281\n"
     ]
    }
   ],
   "source": [
    "# Now let's import the weights obtained from training the same network on Imagenett dataset\n",
    "best_model_path='d:\\\\UTA\\\\Sem-2\\\\ML\\\\CSE6363\\\\assignments\\\\assignment4\\\\lightning_logs\\\\version_15\\\\checkpoints\\\\epoch=8-step=603.ckpt'\n",
    "\n",
    "model = CNNModelwithDropOut.load_from_checkpoint(\n",
    "            best_model_path\n",
    "        )\n",
    "\n",
    "# Add EarlyStopping\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\",\n",
    "                                    mode=\"min\",\n",
    "                                    patience=5)\n",
    "\n",
    "# Configure Checkpoints\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "# Finetuning \n",
    "trainer = L.Trainer(callbacks=[early_stop_callback, checkpoint_callback])\n",
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "print(f\"Restoring from {trainer.checkpoint_callback.best_model_path}\")\n",
    "## Restoring the best checkpoint\n",
    "model = CNNModelwithDropOut.load_from_checkpoint(\n",
    "            trainer.checkpoint_callback.best_model_path\n",
    "        )\n",
    "##Obtaining the test metrics\n",
    "test_result = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
    "\n",
    "print(f\"Best validation loss {trainer.checkpoint_callback.best_model_score.item()}\\nTest loss: {test_result[0]['test_loss']}\\nTest Accuracy: {test_result[0]['test_accuracy']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse6363",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
